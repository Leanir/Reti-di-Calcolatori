# Domande Frequenti

# [[2. Livello Applicativo]]
## Protocolli di livello applicativo:

### [[File Transfer Protocol]]

### Telnet

Telnet è un modello, la cui pila protocollare è costituita da 4 livelli. In cima abbiamo il Livello Applicativo, subito sotto il Livello Di Trasporto, a seguire il livello Internet e il Network Access. Al livello di trasporto si utilizza TCP. Telnet sta per Teletype Network. C’è un client al livello applicativo che si collega alla porta 23 e suddetto client ha la possibilità di collegarsi a qualunque porta del sistema remoto. In Telnet c’è un processo client che controlla il terminale del client, chi manda le informazioni è invece il vero e proprio Telnet Client, che utilizzando TCP comunica con il processo Server di Telnet, il quale, volendo, può comunicare con un’applicazione presente nel server.

### HTTP

Http è un protocollo che serve a trasferire file da una macchina all’altra sotto richiesta. Il client ha un suo applicativo, cioè il browser, che spedisce una richiesta al server web, specificando un indirizzo particolare determinato : URI (Uniform Resource Identifier). In origine il server andava a cercare il file da restituire in ritorno al processo browser. Il sistema è stato potenziato con il concetto di Hyperlink in maniera tale che, se al server veniva richiesta un’informazione aggiuntiva, o il server stesso, o comunque il browser, potevano ricevere informazioni anche da altre macchine. HTTP usa protocollo TCP. Il client invia la richiesta dei file che vuole ricevere, questa richiesta arriva al server che risponde con la lista completa dei file. Tipicamente a richiesta soddisfatta la connessione viene chiusa. La prima versione del protocollo (1.0), prevedeva una chiusura della connessione diretta ogni volta che veniva mandato un singolo file; nel caso quindi della richiesta di più file, veniva utilizzata una connessione per ogni file. La variante successiva (1.1), permette di specificare da parte del client, di tenere aperta la connessione finché il client capisce di aver caricato interamente la pagina web, e solo allora la connessione viene chiusa. 

#### Formato richiesta HTTP 

Nella prima linea di testo c’è un campo dove c’è il metodo richiesto dalla connessione HTTP, dopo c’è uno spazio e a seguire la URL (Uniform Resource Locator), a seguire un altro spazio, successivamente la versione del protocollo e i caratteri utilizzati per il ritorno a capo (cr : carrier return; lf : line feed). Sotto la prima linea ci sono una serie di altre linee chiamate Linee Di Intestazione, dove sono espressi un nome e un valore, variabile e contenuto. La lista di questi nomi seguiti da valori termina quando si ha una linea vuota (contenente solo i caratteri cr e lf). In fondo ci sono altre informazioni aggiuntive (Corpo). Risposta HTTP La risposta ha lo stesso tipo di strutturazione della richiesta, ma cambia la linea in cima. Abbiamo Versione /spazio/ Codice di stato /spazio/ Frase /cr/lf/. Metodi HTTP 1) Get Richiesta per leggere una pagina web 2) HEAD Richiesta per leggere un header di una pagina web 3) PUT Richiesta per conservare una pagina web 4) POST Fare un APPEND a una risorsa nominata 5) DELETE Rimuovere la pagina web 6) TRACE Fare una ECHO alla richiesta in arrivo 7) CONNECT Conservare per uso futuro 8) OPTIONS Fare una query per certe opzioni

### SMTP

Simple Mail Transfer Protocol. È il protocollo della posta. Prima la posta funzionava in modo differente da ora. Ogni utente aveva un account completo caricato su una macchina (server); avevano un account diretto. Ogni utente aveva all’interno del proprio server un file unico messo in una zona controllata dall’amministratore con il nome del log-in dell’utente, dove erano messe una di seguito all’altra tutte le e-mail dell’utente, per cui l’interfaccia che aveva l’utente per accedere, al momento della scrittura, era scomoda poiché conteneva lo storico di tutte le e-mail ricevute. Questa interfaccia permetteva di separare le e-mail e presentarle come se fossero tutte dei messaggi separati. Al momento di trasferire l’email, l’utente, tramite la sua interfaccia, dava le informazioni base (chi era il destinatario, il subject, ecc..) e scriveva un pezzo di testo che veniva messo in append sul file remoto dell’utente. Era (molto genericamente) un modo per fare transfert di file da una macchina all’altra. Per identificare la macchina remota, l’utente doveva comporre un indirizzo composta da log-in dell’utente remoto, @, nome della macchina remota. Si pensò di rendere il sistema più semplice per l’utente. Fu aggiunto un altro applicativo oltre il server di posta, che permetteva di entrare sul file della posta ed effettuare dei trasferimenti ad un altro applicativo separato, chiamato Agent di Posta. Si avrà quindi un computer remoto, con un suo agent di posta che si connette tramite un protocollo di comunicazione via rete, al server di posta, interagisce con il file e poi invia il file. Fu poi rimosso il login, inserendo i profili dentro il server, trasformando l’utente in account locale al server, e furono aggiunti una serie di account virtuali autorizzati con log-in e password.

# [[3. Livello di Trasporto]]

## Transmission Control Protocol

Protocollo affidabile e orientato alla connessione con congestione (ottimizzazione) della banda e ottimizzazione del flusso.

Impiega tempo e risorse in quanto molto più complesso dell'UDP. Si occupa di fare l’indirizzamento della porta (multiplexing). Stabilisce la connessione, la gestisce e la chiude. Si occupa di gestire i dati e di fare il packaging dei dati. Garantisce l’affidabilità e una sorta di qualità del servizio. Gestisce il controllo del flusso e il controllo della congestione. Cerca ovviamente di evitare la congestione, anche se c’è differenza tra controllo del flusso e controllo della congestione.

Non si preoccupa troppo delle applicazioni, quindi bisogna mettere della applicazioni sopra TCP per lavorare meglio. Non è molto sicuro. Non gestisce i limiti dei messaggi, prevede un flusso ininterrotto di byte. Non garantisce la comunicazione.

Da un punto di vista del programmatore, il processo che deve mandare dei dati si interfaccia esclusivamente con un modulo con delle funzioni di libreria che utilizzano le socket; le quali si interfacciano con il vero modulo TCP che ha al suo ingresso un Buffer di ricezione. Questo vuol dire che il processo non controlla direttamente la comunicazione TCP tramite le socket, le socket depositano le comunicazioni in un buffer, poi sarà il protocollo TCP a svuotare periodicamente il buffer creando dei segmenti e facendoli arrivare a destinazione. A destinazione i dati vengono messi in un nuovo buffer e a seconda del funzionamento del sistema remoto vengono passate al processo remoto. Quindi il tutto può essere immaginato come un qualcosa per cui il processo mittente e il processo di destinazione, sono legati tra di loro da un !tubo” dove vengono inseriti singoli byte senza alcuna suddivisione ben precisa, un flusso di dati tra mittente e destinatario insomma. Il flusso non è regolato dai processi ma è regolato dal protocollo TCP a seconda del funzionamento di tutto il sistema. I dati vengono inviati a blocchi, vengono poi accodati, man mano che transitano potrebbero arrivare al destinatario con uno spezzettamento che non coincide con quello generato dal mittente. Può capitare che il mittente abbia necessità di mettere ordine tra i dati spediti in un qualche modo, questo compito però va fatto eseguire dall’applicativo.

TCP è bidirezionale ed è quindi attivo sia in ricezione, sia in trasmissione contemporaneamente. Si inizia dallo stato closed. Il sistema può mettersi in ascolto oppure può cercare di attivare una comunicazione, ovviamente il primo ad essere attivato può essere la macchina remota messa in ascolto, mettendosi in uno stato di listen, dopo di che rimane bloccato in attesa di ricevere un’informazione dalla controparte. La controparte può inizialmente essere in uno stato di chiuso, e in maniera attiva cerca di attivare la connessione e manda un segnale, in particolare chiamato SYN, alla controparte. Se tutte e due a questo punto sono d’accordo nel continuare, allora la connessione è stabilita, le due macchine sono in contatto fra di loro; a questo punto la comunicazione è perfettamente bidirezionale. Nella fase di chiusura, essa può essere fatta da entrambe le parti, sia quindi da chi ha aperto la connessione, sia chi ha ricevuto la connessione. Di fatto, succede che una delle due macchine decide di mandare il segnale di chiusura e procede, l’altra macchina riceve l’informazione di richiesta di chiusura, fa alcune operazioni, poi di fatto chi ha iniziato la richiesta, entra nello stato di wait, e poi torna nello stato di chiuso.

Cosa prevede il formato del pacchetto TCP:
Non è definita la dimensione, dispone, oltre che di un campo variabili, anche di un campo opzioni, la cui dimensione non è prestabilita, ma è variabile. La dimensione dei campi N.porta sorgente, N.porta destinazione, Numero di sequenza, numero di riscontro, Lunghezza intestazione, finestra di ricezione, checksum, puntatore di dati urgenti, sono di lunghezza fissa, 32 bit; 5 parole da 32 bit. Dovrebbero essere 20 byte, che sono 4 per ogni riga. Abbiamo il numero di sequenza. Esso è un intero a 32 bit che identifica ogni singolo byte della connessione. Il tutto viene servito come un flusso di byte tra mittente e destinatario. Di fatto dobbiamo avere quindi un identificativo pe rogni singolo byte presente nella connessione, per questo hanno deciso di mettere questo intero a 32 bit. Successivamente abbiamo un campo ACK, che è relativo al numero di sequenza in direzione opposta. Il numero di sequenza identifica il primo byte dei dati che vengono spediti, questo vuol dire che (esempio) 100, identifica il primo byte di una sequenza di (esempio) 2048 byte. Se 100 è il primo, l’ultimo sarà 2147, ma non ha senso mandarlo visto che sapendo la posizione del primo, si sa automaticamente la posizione dell’ultimo. L’ACK, che viene spedito indietro, fa riferimento esattamente al numero di sequenza spedito in direzione opposta indicando quel è il byte successivo che il ricevitore è disponibile a ricevere. Di questi 2Kbyte, ricapitoliamo, il primo è identificato con il valore 100, l’ultimo è identificato con il valore 2147; dato che partiamo da 0 l’ultimo è il ValoreTotale – 1. Quindi il ricevente sta dicendo “va bene, li ho ricevuti tutti, vorrei che il prossimo byte da ricevere abbia il numero di sequenza 2148”. Infatti nella comunicazione successiva il numero di sequenza inizia con 2148, esattamente quello riportato dall’ACK. Il numero di sequenza identifica il primo byte presente nei dati, il numero di riscontro non è relativo a questa comunicazione ma è relativo alla comunicazione precedente che è legata al pacchetto preso in esame. Ricapitolando, il numero di sequenza identifica i dati che stiamo mandando, ne identifica il numero del primo byte(numero di sequenza del primo byte presente nei dati), sapendo quanti dati ci sono, si può avere anche l’informazione sul quale sia il valore di sequenza dell’ultimo byte. Il numero di riscontro invece è relativo al numero di sequenza della comunicazione in verso opposto, fa quindi riferimento al pacchetto che è stato spedito al contrario rispetto alla comunicazione. Non sono quindi riferimento tra loro, l’uno con l’altro, ma sono numeri relativi a due comunicazioni differenti. Si usa questa tecnica per mischiare la comunicazioni da A verso B, e da B verso A; in particolare la comunicazione da A verso B trasporta le risposte della comunicazione da B verso A, per questo motivo sono presenti nella stessa intestazione. La Lunghezza dell’intestazione dice quanto è grande la sezione, in valori da 32 bit. Ci sono poi alcuni bit non usati, 6 bit non usati; seguono successivamente una serie di bit, che sono denominati flag, perché sono divisi in bit singoli. In particolare, abbiamo syn, che avvisa che sta per essere avviata una connessione con la controparte, viene settato per due pacchetti, e poi non viene settato più(rimane a 0). FIN è il flag che indica la volontà di chiudere la connessione. RST (reset) è un flag di reset, per resettare in caso succeda qualcosa di non previsto. PSH (push) è un flag che viene inserito per dire di svuotare il prima possibile il buffer di spedizione, viene utilizzato quindi per tutte le macchine che seguono la catena di comunicazione. ACK è un flag che serve per dire che il campo acknowledgment number, è da considerare come valido, altrimenti viene totalmente ignorato, significa che la grandissima maggioranza delle volte, il flag è settato a 1. URG (urgent) è un flag che serve per dire alla controparte che ci sono dei dati urgenti da valutare prima di tutti gli altri; è legato anche allo urgent pointer, presente nell’intestazione. Dato che ho un flusso costante di byte, può capitare a un certo punto che il mittente inserisca delle informazioni che devono essere valutate immediatamente annullando, o bypassando tutto ciò che era stato spedito prima, però se quello che c’era prima non viene valutato dal ricevente, i dati urgenti non vengono mai valutati; avendo previsto questa cosa, viene quindi inserito un contatore dati urgenti, un flag relativo a questo campo, che serve a dire alla controparte di ignorare ciò che veniva prima del dato urgente, fa prendere subito i dati segnalati e si procede a seconda di quello che viene indicato dai dati in questione. Il campo checksum è identico ad UDP come struttura, ma potrebbe non essere controllato durante le varie implementazioni, soprattutto dai router di transito perché, intanto i router di transito non vanno sempre a vedere nel livello di TCP, il problema principale è che il controllo sulla correttezza dei campi viene fatto al livello di Data Link, quindi questo controllo risulta inutile. Il campo window size serve per realizzare il controllo del flusso. Il campo opzioni serve per negoziare la massima dimensione del segmento (MSS) accettabile da parte di tutta la comunicazione. Il checksum controlla tutto, sia l’intestazione IP, sia l’intestazione TCP, sia i dati, e fa alla fine un checksum totale.

### Apertura e chiusura delle connessioni

La macchina A vuole aprire la comunicazione con la macchina B, se la macchina A è il client la macchina B è il server; il server deve essere già attivo prima che il client comincia a spedire il primo pacchetto. A mete un numero di sequenza, non è importante il contenuto del numero, il campo ACK deve essere relativo alla comunicazione precedente, solo che non c’è stata una comunicazione precedente, quindi in qualche modo deve poter dire all’host di destinazione “guarda che questo campo non lo devi prendere in considerazione”, e lo fa con il flag ACK, che in questo caso viene messo a 0. Nella prima comunicazione viene messo il flag syn a 1. Quindi abbiamo syn=1, ACK=0. L’host B riceve la comunicazione, risponde impostando il campo ACK con il ValoredellaSequenza+1, il numero di sequenza sceglierà un suo numero, e risponde dicendo “ok, anch’io voglio aprire la connessione, però il flag ACK viene posta a 1”, per dire che quel campo deve essere preso in considerazione (è un campo valido). La risposta è il numero di sequenza peri all’ACK precedente, il numero di ACK è il valore del NumeroDiSequenzaPrecedente+1, e il flag ACK ovviamente è posto pari a 1, il flag syn a questo punto è 0. Da questo momento la comunicazione può essere considerata aperta. Scenario che può capitare Sia 1, sia 2, vogliono aprire la connessione; queste vengono considerate contemporanee perché la creazione della connessione da 2 verso 1 viene fatta qualche istante prima che arrivi la richiesta da 1 verso 2. Quindi anche se non sono temporalmente coincidenti, vengono considerate coincidenti (nel protocollo) perché non sono collegate tra loro le operazioni, per ciò vengono considerate contemporanee. Viene definito quindi Handshake a tre vie; se fossero 4 vie, abbiamo che, quando l’host 1 riceve una sequenza, dice “io ho chiesto di aprire una connessione, col valore x, la controparte contemporaneamente mi ha chiesto di aprire la connessione con il valore y”, questi sono i due valori necessari per aprire la connessione, e risponde con il suo numero di sequenza e ACK+1. Flag syn settato, ACK settato. L’host 2 fa esattamente lo stesso tipo di lavoro, ovviamente invertendo i valori rispetto all’altra connessione. Handshake a tre vie A un certo punto può capitare che spunta un messaggio perso nella rete, una vecchia richiesta di creazione della connessione spunta fuori quando in realtà l’host 1 non ha più interesse ad aprire la connessione verso l’host 2. È uno scenario raro, ma è uno scenario possibile. Non possiamo fidarci del fatto che non esistano messaggi che si perdano nella rete e che spuntino improvvisamente senza motivo. Ovviamente l’host 2 non può sapere che questo messaggio è vecchio, non c’è un timestamp dei pacchetti e in ogni caso non avrebbe diritto a negare una richiesta del genere. Quindi risponde correttamente come dice il protocollo, l’host 1 invece rigetta la connessione. Non la apre, informando che l’host 2 non è più disponibile ad aprire la connessione. Altro scenario che può capitare Arriva una richiesta vecchia, host 2 tenta di aprire; c’è un altro vecchio messaggio che tenta di aprire una accettazione di richiesta di connessione. Capita quindi che ci sono due richieste col numero di sequenza (esempio) x, il problema sta nel valore dell’ACK, che è relativo al numero di sequenza della connessione precedente. Il vecchio messaggio avrà un numero di ACK diverso dal numero di sequenza x. A questo punto l’host 1 rigetta la connessione. Viene chiamato Handshake a tre vie perché comporta una sfida, tra la macchina 1 e la macchina 2. La sfida è banale, ed è un qualcosa che è alla base di alcuni protocolli di verifica di correttezza. La sfida banale, è la seguente. “io ti sto mandando un numero x, tu mi devi rispondere con lo stesso numero x oppure con un altro valore che è legato in ogni caso al valore x, per esempio x+1”. Questa sfida può essere vinta solamente se questo messaggio arriva prima della risposta. La sfida diventa particolare quando abbiamo che due messaggi non sono collegati tra loro. Se un messaggio contiene un certo valore di ACK, notiamo che comunque il flag ACK viene settato a 0 per dire “lascia perdere perché quello è un valore che non devi considerare”. Quindi i due messaggi che si scambiano gli host non sono legati fra di loro, ma sono totalmente indipendenti. I successivi messaggi sono invece legati ai precedenti presi in esame, poiché sono una diretta conseguenza di questi ultimi. Abbiamo un messaggio da A a B, che una volta ricevuto da B, viene lanciata la sfida. La risposta viene dopo la domanda, infatti il numero di sequenza viene riportato nel valore di ACK, questa è la sfida, quindi sicuramente il messaggio è posteriore. Il messaggio che partito da B arriva ad A, dice “ok, ora mi devi rispondere”, lancia un’altra sfida, “ora mi devi rispondere con un valore di ACK che contiene il valore y”, la sfida ovviamente deve essere persa, o almeno, viene persa perché nel caso di un messaggio vecchio, non sappiamo da dove esso arrivi, e tra l’altro non è legato al precedente, quindi succede inevitabilmente, appunto, che la sfida viene persa. Non solo arriva un messaggio di reject, ma l’host 2, che si vede arrivare la risposta, capisce che è successo un problema. Per questo host 2 chiude la connessione (la sfida non è stata vinta). L’apertura funziona in questa maniera. Host 1 sfida Host 2, e Host 2 sfida Host 1. La sfida si definisce tale, perché essa è semplicemente : “tu sei attivo in questo momento, sei presente nella rete e rispondi, oppure i messaggi che stanno girando sono messaggi vecchi, e quindi devono essere ignorati?” . Le sfide sono una questione di temporalità della sequenza dei messaggi. Il messaggio tre deve essere posteriore al messaggio due, e il due deve essere posteriore al messaggio uno. Se questa sfida viene vinta vuol dire che 1 e 2 vogliono in quell’istante parlare fra di lor e a questo punto inizia la connessione vera e propria. Il protocollo funziona a una risposta binaria. Apertura della connessione – reject (chiusura) della connessione. Se dovessero esserci problemi di comunicazione ovviamente la connessione banalmente non viene aperta. Se non ci sono problemi di connessione e tutto fila liscio, la connessione viene aperta. E può cominciare il dialogo tra le due parti.

### Controllo della congestione vs Controllo del Flusso

Controllo del flusso vs Controllo della Congestione (Slide del rubinetto, you know the one [n98-Trasporto6.pdf], non ne vale la pena di metterla. Si riferisce alla parte a sinistra) Fast Retransmit e Timer sono stati pensati per migliorare la gestione della rete. Nel senso che questo meccanismo evita che il timer troppo grande fermi la connessione e si cerca di riprendere la comunicazione il prima possibile generando un solo pacchetto ritrasmesso in più rispetto ad avere la scadenza del timer. Un timer efficiente calcolato bene evita anche le trasmissioni inutili di pacchetti che sono solo in ritardo ed evitano di mettere timer fissi troppo grandi che porterebbero come detto poc'anzi a ritardi nella comunicazione. Non sono direttamente legati alla gestione della congestione ma di fatto servono anche a migliorare / evitare di arrivare troppo presto alla congestione. Controllo del flusso: Evitare di far viaggiare troppi pacchetti se a destinazione non c’è lo spazio per memorizzarli accuratamente. (Buffer di Ricezione che viene svuotato dall’applicativo). Non è un problema se il buffer è piccolo se l’applicativo lo svuota continuamente senza problemi. L’informazione sul buffer di ricezione è limitata dal ritardo di comunicazione tra mittente e destinatario, in quanto è inserita in un pacchetto. Quindi, quando viene mandato dal destinatario il pacchetto con l’informazione “Il buffer è pieno”, ci sono ancora dei pacchetti in viaggio nel canale che sono già stati spediti dal mittente, e se il buffer è pieno (e non viene svuotato in tempo, magari per fortuna qualche pacchetto passa e allora vengono mandati ACK duplicati), tutti questi pacchetti verranno buttati via. Il controllo del flusso, pur essendo molto robusto e logicamente corretto, soffre dalla limitazione fisica della distanza tra mittente e destinatario (Il problema descritto sopra si manifesta in modo molto pesante nelle comunicazioni da un capo del mondo all’altro). In questi casi non basta il controllo del flusso e quindi non può che essere considerato “Una pezza al problema” invece di una vera e propria soluzione. Il problema del controllo del flusso è che il destinatario non ce la fa a smaltire i pacchetti che ricevono in tempo, e allora il mittente va informato di smettere, momentaneamente, di trasmettere. Perché altrimenti i pacchetti vanno buttati. L’unico sistema che abbiamo è di usare il canale di comunicazione per dire al mittente “smettila che il ricevente non ce la fa”. Non è perfetto perché se c’è un forte ritardo tra i due si perdono tutti i pacchetti in viaggio mandati dopo la richiesta di pausa. Questi pacchetti genereranno tutti timeout nel mittente, ma il mittente è bloccato e quindi dei timeout se ne frega perché non ha più il permesso di spedire. Definizione di Congestione (Si riferisce sempre alla slide del rubinetto, ma la parte a destra) Il ricevitore ha una capacità large ma il canale ha una “strozzatura”. Il traffico arriva, ma arriva piano piano. È una congestione o un rallentamento? Le due cose sono molto differenti. La congestione non è rallentamento. [Qui metterò il riassunto di circa 4 ore di spiegazione, perché per ora ha detto solo che la congestione è complicata] Tentativo di definizione n1: “Il Controllo di Congestione è un meccanismo end-to-end che tenta di capire cosa succede nel canale, cosa che (le macchine che comunicano) non possono fare, sperando che le decisioni prese influiscano a livello di rete. Questo è il problema grosso del controllo della congestione.” Tentativo di definizione n2: “La congestione si ha quando il traffico di livello di trasporto è fatto da pacchetti ritrasmessi all’infinito” (che non contribuiscono affatto alla comunicazione ndr)” Tentativo di definizione n3 (dalla lezione 9): “La congestione è dovuta al fatto che ci sono tanti pacchetti ritrasmessi e pochi pacchetti nuovi che arrivano a destinazione. Cioè la rete è piena di pacchetti (inutili) che sono copie, di copie, di copie…” Primo Scenario: buffer illimitato Ci sono due comunicazioni, A<->C e B<->D. Tutti i dati di queste due comunicazioni vengono scambiati tramite un collegamento condiviso. La prima ipotesi è che il router in mezzo ha un buffer infinito. I pacchetti non possono perdersi per capienza massima. Il link in uscita ha comunque una larghezza di banda, quindi i pacchetti vengono comunque accodati se vengono mandati troppo velocemente. In pratica, il buffer è infinito, ma l’uscita è limitata. I pacchetti potrebbero quindi essere accodati all’infinito. Questo è un problema di teoria dei grafi. Ecco la soluzione: Supponendo che le connessioni siano “intelligenti”(???) si suddividano equalmente la banda a disposizione λIn : Traffico generato λout : Traffico che arriva a destinazione Fino a quando siamo al di sotto di R/2 (cioè metà della banda a disposizione perché se la stanno dividendo) allora tutto il traffico generato arriva in uscita senza problema alcuno. Quando il traffico in ingresso diventa superiore alla larghezza di banda in uscita allora λout si ferma al valore massimo. λin può comunque crescere ma se dovesse farlo allora i pacchetti verranno accodati all’infinito. Ciononostante in un tempo infinito usciranno dal buffer. Finché il traffico generato è inferiore al traffico in uscita funziona, seppur con ritardo enorme di comunicazione. BENE QUESTA NON È CONGESTIONE È SOLO UN SEMPLICE RITARDO. Si hanno ritardi enormi ma non è congestione. =====SECONDA PARTE (con registrazione)===== Secondo Scenario: buffer reale Abbiamo: λIn: Traffico generato a livello applicativo λ’In: Traffico generato a livello applicativo + livello di trasporto Questi dati non coincidono. Infatt λ’In = λIn+ dati ritrasmessi. λ’In >= λIn Sempre. Saranno uguali se non ci sono ritrasmissioni. Se ci sono dati che vengono ritrasmessi a causa di una perdita allora λ’In sarà maggiore. Queste ritrasmissioni avvengono perché adesso il buffer è limitato e quindi dei pacchetti verranno buttati nel caso in cui si dovesse riempire. Il tempo di transito per un pacchetto che ha la fortuna di trovare uno slot libero nel buffer ha un massimo ben preciso che dipende dalla: ● Grandezza del buffer ● Velocità di elaborazione del router. Non c’è più il problema del ritardo infinito. “Paghiamo” questo tetto massimo al ritardo di comunicazione con delle ritrasmissioni (aka spreco di banda). I tempi di transito sono finiti. I pacchetti duplicati non sono utili per il λout , anche se il traffico a livello di trasporto aumenta, il traffico a livello applicativo si ferma perché non ci sono più nuove informazioni utili. La congestione si ha quando il traffico di livello di trasporto è fatto da pacchetti ritrasmessi all’infinito che non contribuiscono affatto alla comunicazione. L’Applicativo continua a mandare ACK duplicati dicendo che il pacchetto è arrivato, ma il sistema continua ad andare in timeout, e quindi a rispedire pacchetti vecchi, e man mano che la coda si riempie di duplicati l’applicativo non riesce ad andare più avanti. La congestione quindi può arrivare al punto critico di alimentare se stessa. Una situazione dove quindi λ’In >> λIn e λout è prossimo allo zero. I pacchetti duplicati ci sono perché scade il timer (anche se il povero pacchetto era solo in ritardo). Il pacchetto duplicato va a fare ancora più confusione nella rete e fa ancora più confusione. In questo scenario paradossalmente se aumentiamo la memoria al buffer peggioriamo questa situazione. Contrario a quanto si potrebbe pensare inizialmente, converrebbe avere buffer piccoli. (Cosa che comporta altri problemi…) (-- Nel caso in cui lo chiedesse, esiste una politica di gestione delle code chiamata RED: Random Early Detection (Wikipedia), che butta “quasi a caso, in realtà non proprio a caso” i pacchetti, e migliora leggermente la situazione. --) Terzo Scenario :4 Host che si creano congestione a vicenda Ante Scriptum: Tutto questo paragrafo va letto in chiave di quest’immagine, molte frasi sono prese 1:1 dalla registrazione su MS Stream. Il traffico in andata e in ritorno seguono via diverse molto particolari e strane. Sostanzialmente le connessioni si disturbano tra di loro, perché attraversano stessi router e buffer. È importante sottolineare la definizione di connessione Asimmetrica (ADSL => la A sta per Asimmetrica): Di norma in una connessione asimmetrica le velocità di download e upload sono differenti. La velocità di upload contiene gli ACK delle connessioni in download. Se si dovesse saturare il canale di upload gli ACK non riescono ad arrivare a destinazione: il mittente che usa il canale di download per noi invia duplicati perché gli ACK non stanno arrivando. Quindi il traffico utile sarà limitato perché il canale di upload viene saturato da qualcun altro. Questo fatto blocca la comunicazione su ENTRAMBI i canali, anche se separati. Controllo della Congestione ATM (accenno) Dunque si ha congestione quando i buffer dei router si riempiono e il traffico viene limitato e infine bloccato. I router non potrebbero dunque avvertire gli host che i propri buffer si stanno riempendo criticamente e dunque di rallentare la velocità di trasmissione? ATM permetteva di farlo (non è parte di programma), funzionava bene ma TCP non lo può fare perché TCP mette in diretta comunicazione (logica) i livelli di trasporto: non vede il livello di rete. AIMD Quando posso pensare che c’è un problema? Quando scade un timer. Qui subentra la fase AIMD: Additive Increase, Multiplicative Decrease. Le due connessioni di trasporto non hanno idea della velocità a cui possono andare. Quando inizia a spedire pacchetti, l’ideale è riempire il più possibile il canale senza saturarlo. Il problema sta nei router che stanno in mezzo e i loro buffer: idealmente vorremmo il canale tutto pieno (massimo throughput) e i buffer in ingresso dei router vuoti (minima congestione). Con AIMD mando i primi pacchetti e va tutto bene, arrivano correttamente. Allora provo a mandarne qualcuno in più. Ogni step aumento di una quantità fissa (lineare) il numero di pacchetti mandati finché non succede qualcosa, ad esempio la perdita di un pacchetto. Appena succede questo qualcosa mi fermo e decremento dividendo per una certa quantità, ad esempio per 2. AIMD è una delle tante politiche utilizzate nel controllo di congestione TCP. Ha una crescita lenta ed è molto cauto. Nella fase di incremento additivo sto molto al di sotto della capacità massima possibile per molto tempo. Slow Start Un altro approccio che si usa in concomitanza con AIMD All’inizio mando un pacchetto, arriva l’ACK. Ne mando due. Mi arrivano ambo gli ACK. Ne mando quattro, e così via… Questo è un approccio esponenziale e molto più “brutale. Quindi si usano insieme. (prima aveva detto che Slow Start era un’implementazione di AIMD, sbagliando platealmente) Accenno di controllo della Congestione in TCP Inizio in Slow Start: cioè vado in incremento esponenziale: 1,2,4,8,16… fino a raggiungere una certa soglia (che verrà spiegato dopo, don’t worry), dopodichè vado in approccio AIMD lineare. Al primo timeout torno di nuovo a 1 e ricomincio, ma ora ho modo di calcolare la soglia. L’idea è che la soglia la pongo a metà di dove c’è stato il timeout: Se avviene a ‘24’ (unità generiche per ora), allora la pongo a ‘12’. Appena la supero proseguo di nuovo in AIMD. 3 ACK duplicati sono un altro problema, viene discusso dopo. Questo è l’approccio in Tahoe. Dettagli successivamente.

Il controllo della congestione: La congestione è dovuta al fatto che ci sono tanti pacchetti ritrasmessi e pochi pacchetti nuovi che arrivano a destinazione. Cioè la rete è piena di pacchetti (inutili) che sono copie, di copie, di copie… - Più pacchetti ci sono fermi nei buffer di transito più la rete diventa congestionata. (parentesi: I buffer di transito NON sono i buffer di mittente <-> destinatario, ma tutti i buffer dei router che stanno nella rete nel percorso da mittente a destinatario, per questo la congestione è un problema così ostico: non ha a che fare con mittente e destinatario, che sono collegati logicamente a livello di trasporto, ma ha a che fare con i router a livello di rete [e quindi invisibili a mitt-dest]). Idealmente vogliamo che il mittente stia sempre a spedire e quindi riempire il canale, ma tenere vuoti i buffer dei router intermedi (cioè che riescano a smaltire il traffico). Tutti questi buffer intermedi sono indipendenti e non coordinati tra loro e potrebbero anche avere traffico di “disturbo” da altre fonti. I segnali d’allarme della congestione sono il timeout o 3 ACK duplicati. Il mittente può recepire questi due segnali e capire che la comunicazione non sta andando ‘liscia’. TCP-Mittente inizia a rallentare quando riceve questi segnali, perché ovviamente non può intervenire sulla rete direttamente, sperando che questo rallentamento abbia un effetto positivo (buffer che si svuotano, etc). Come riprendere a ri-aumentare il traffico? Non sapendo qual è il throughput max che può viaggiare in termini di banda / pacchetti può fare solo una prova. Se un certo throughput può essere sopportato dalla rete allora prova ad aumentarlo. Esso viene aumentato in termini di pacchetti spediti (con le modalità descritte precedentemente: Slow Start e poi AIMD). Tahoe vs Reno TCP Tahoe implementa: ● Slow Start ● Congestion Avoidance ● Fast Retransmit TCP Reno implementa: ● Slow Start ● Congestion Avoidance ● Fast Retransmit ● Fast Recovery Slow Start: Si parte da 1, poi si raddoppia se tutto va bene, 2, 4, 8 con un incremento esponenziale. Ovviamente se non mi fermo in qualche modo saturerei subito la rete e andrei incontro ad un timeout / triplo ACK. Allora rallento una volta superata la Slow Start Threshold ( ssthresh ) entro in Congestion Avoidance (CA) e aumento linearmente (AIMD). Ovviamente arriverò ad avere dei problemi anche così, ma molto più lentamente. Dato che le due versioni di TCP prese in esame implementano il meccanismo di Fast Retransmit posso trattare in modo particolare i 3 ACK duplicati e prenderli come sintomo che è successo qualcosa nella rete. La differenza grossa tra Tahoe e Reno è il meccanismo di Fast Recovery, che Reno implementa. Tahoe in entrambi i casi (timeout / 3 ACK dup) si ferma totalmente e riparte in Slow Start, setta ssthresh a metà del valore precedente e riparte. Reno invece fa una considerazione differente, dice “Ok, è successo qualcosa ma non è grave, perché gli ACK comunque mi sono arrivati, non è un timeout vero e proprio.” Si rallenta per dare possibilità alla rete di smaltire un po’ di traffico, ma si riprende a partire dalla soglia con AIMD.(??? Controlla kurose perché ha spiegato tutto senza le formule di CWND e MSS)

# [[4. Livello di Rete]]

[[Ese E01]] <- dovrebbe essere qui

## IPv4

### Indirizzamento inter lan e 

### indirizzamento intra lan  

## IPv6

IPv6 IPv6 deriva dai limiti di IPv4: ● Esaurimento dello spazio degli indirizzi (4 miliardi: allocati male inizialmente, estrema diffusione di Internet negli ultimi anni, e Internet Of Things). ● Scalabilità del Routing (dovrebbe essere fatto in maniera geografica, per esempio puramente arbitrario: In america mettiamo gli indirizzi che iniziano per 1, in Canada per 11, in Europa per 2, etc…, Con IPv4 questo non succede e IP numericamente vicini potrebbero essere agli antipodi). ● Nuovi Servizi (Servizi che potevano essere implementati hanno dimostrato dei limiti su IPv4). Oggi gli indirizzi IPv4 sono completamente finiti. Inoltre, le entry nelle tabelle di routing iniziano a diventare “pesanti” (troppo complesse), un routing complesso è un routing lento. Nuovi servizi di IPv6: ● Sicurezza ● Autoconfigurazione (Plug & Play) ● Gestione della Quality of Service (Prevista in IPv4 ma mai usato / ignorato) ● Indirizzamento Multicast (tanti host all’interno di una stessa rete) ● Indirizzamento host mobili

### Header IPv6

#### Rimossi: 

● ID, Flags, Offset (si sconsiglia la frammentazione in IPv6) ● ToS (inutilizzato), Header length (Ci si sposta a un header a dimensione fissa) ● Header Checksum (considerato ridondante, non serve più) HLen è stato rimosso perché con l’intestazione fissa il router è sicuro che deve guardare solo i primi 40 Byte senza sorprese particolari dopo. Le opzioni vengono gestite in un altro modo.

#### Cambiati

● Total Length → Payload Length ● Protocol → Next Header (gestito diversamente) ● TTL → Hop Limit (gli viene dato il nome corretto, 1 Byte) Total Length diventa Payload Length dato che è diventata l’unica variabile nell’intestazione.

#### Aggiunti

● Traffic Class ● Flow Label


## Protocolli di routing

### Distance Vector (vedi esempio in excalidraw)

### LSR

# [[5. Data Link Layer]]

Rilevazione e correzione degli errori

Indirizzamento a livello DL

# [[6. Livello Fisico]]

IEEE 802.3

# Poveri Studenti sono stati bocciati per queste domande

Fonte:

```cardlink
url: https://gitlab.com/UNICT-DMI/Triennale/SecondoAnno/reti-di-calcolatori/-/blob/master/Appunti/De-Brevitate-Retis_v133.pdf?ref_type=heads
title: "Appunti/De-Brevitate-Retis_v133.pdf · master · UNICT-DMI / Triennale / SecondoAnno / Reti di Calcolatori · GitLab"
description: "GitLab.com"
host: gitlab.com
favicon: https://gitlab.com/assets/favicon-72a2cad5025aa931d6ea56c3201d1f18e68a8cd39788c7c80d5b2b82aa5143ef.png
image: https://gitlab.com/assets/twitter_card-570ddb06edf56a2312253c5872489847a0f385112ddbcd71ccfa1570febab5d2.jpg
```

Domande che spesso richiedono dettagli che vengono esposti solo a lezione e non si trovano affatto (o difficilmente) sui libri

## Possiamo avere un throughput maggiore di R? Tecnicamente sì. Come?

Questa domanda è legata alla dimostrazione della fairness

## I PROBLEMI DI ARP

## LAN Interconnesse

## Campo Flow Label

Quando il campo Flow Label è settato (contiene un numero) i router di transito non vanno più a guardare l’indirizzo di destinazione, per quanto riguarda il routing, ma guardano nell’apposita tabella per capire come fare andare avanti il traffico. È un modo per inserire il sistema di circuito virtuale in IP (che migliora molto il routing rispetto al servizio Datagram puro di IPv4). Quindi i pacchetti di uno stesso “stream” seguiranno tutti la stessa strada in ordine. Non ha preso molto piede e spesso viene ignorato.

## Distanza di Hamming

## RIDONDANZA (la roba coi cerchi e intorni di distanza $d = 1$)
![[Pasted image 20240616131950.png]]